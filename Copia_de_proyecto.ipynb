{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copia de proyecto",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joseluisgarciad/Arbol/blob/master/Copia_de_proyecto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4Oc9OyMEhpY"
      },
      "source": [
        "## 1. Memoria del proyecto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Jl9Et-2Cw-M"
      },
      "source": [
        "### **1. Introducción**\n",
        "\n",
        "Objetivo del proyecto:\n",
        "\n",
        "Nuestro cliente es propietario de una plataforma educativa online, con presencia internacional. Entre su oferta, encontramos contenido especializado en el área de tecnología: programación, Big Data, desarrollo web, Apps móviles, etc. \n",
        "\n",
        "El servicio que llevaremos a cabo consistirá en orientarles acerca de cuáles de estos productos son los más demandados. Para ello nos basaremos en datos de registros y tendencias de uso, que pondrán énfasis en la información obtenida “en tiempo real”. De esta manera, nuestro cliente podrá tomar las decisiones más adecuadas de cara a su oferta educativa del próximo año. La optimización de sus recursos estará respaldada por nuestra tecnología y experiencia en Big Data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AU7iVL7eDXWE"
      },
      "source": [
        "###**2**. Diseño de arquitectura del proyecto\n",
        "\n",
        "\n",
        "\n",
        "Como respuesta a las demandas de nuestro cliente, hemos desarrollado una arquitectura concreta, en la que distintos sistemas y herramientas nos llevarán a alcanzar la solución más adecuada, que nos muestre datos con significado para su compañía y sus siguientes objetivos estratégicos.\n",
        "\n",
        "Hemos decidido dividir la estrategia en dos itinerarios. En primer lugar, para conocer qué productos son más demandados por sus potenciales clientes, obtuvimos registros de actividad en los foros más utilizados por programadores a nivel mundial (Stackoverflow.com). \n",
        "También se necesita saber de que lenguajes/tecnologias se habla en Twitter, en concreto se necesita saber en que paises y en que idiomas para realizar una estadistica.\n",
        "A continuación hacemos un esquema de las arquitectura de ambos procesos.\n",
        "\n",
        "\n",
        "\n",
        "**2.1 BASES DE DATOS DE ORIGEN STACKLITE**\n",
        "\n",
        "- Realizamos la ingesta de datos mediante Sqoop, a través de Hive, llevando las Bases de datos originales a nuestro servidor Hdfs.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1zSTdtMoOpn"
      },
      "source": [
        "sqoop import --connect jdbc:mysql://10.0.4.201:3306/stacklite --username training --password training --table questions --hive-import"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Neeh-dMroQN8"
      },
      "source": [
        "* Trabajamos estos datos a través de Spark en Python. Para ello, ejecutamos el siguiente archivo de configuración:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vG-VOqTapvH7"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark.stop()\n",
        "spark = (SparkSession\n",
        "         .builder\n",
        "         .appName(\"Python Spark SQL Hive\")\n",
        "         .config(\"spark.sql.warehouse.dir\", \"user/hive/warehouse\")\n",
        "         .config(\"hive.metastore.uris\", \"thrift://ip-10-0-4-11.eu-west-1.compute.internal:9083\")\n",
        "         .enableHiveSupport()\n",
        "         .getOrCreate()\n",
        "         )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzZvnnW4p5ET"
      },
      "source": [
        "Transformamos las bases de datos, unificándolas por el criterio común de id. Para las consultas, utilizamos la extensión de SparkSQL."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCUFfG7vqP6S"
      },
      "source": [
        "df_q = spark.sql(\"\"\" SELECT * FROM project_questions \"\"\")\n",
        "df_q.printSchema()\n",
        "\n",
        "df_qt = spark.sql(\"\"\" SELECT * FROM project_question_tags \"\"\")\n",
        "df_qt.printSchema()\n",
        "\n",
        "df_qt.registerTempTable('qt1')\n",
        "sql_qt1=spark.sql('select * from qt1')\n",
        "sql_qt1.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwSW1urLqhYN"
      },
      "source": [
        "Con el siguiente comando, vemos el conteo de, en general, cuales son los lenguajes más demandados por los usuarios de los foros de esta plataforma:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCWYAeERq7qt"
      },
      "source": [
        "df_qt_tmp=spark.sql('select qt1.tag, count(*) as contador from qt1 group by qt1.tag order by contador desc limit 20')\n",
        "df_qt_tmp.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBDCyFBsq-8R"
      },
      "source": [
        "Sin embargo, creemos que un criterio más relevante para nuestro cliente sería obtener datos más actuales. Ordenaremos nuestros resultados por fecha, de más actual a más antiguo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DimWZLgtrb5d"
      },
      "source": [
        "df_q.registerTempTable('q1')\n",
        "\n",
        "df_fecha = spark.sql('select * from qt1, q1 where q1.usuario = qt1.id limit 20')\n",
        "df_fecha.show()\n",
        "\n",
        "df_fecha1 = spark.sql('select id,tag,creationdate from qt1, q1 where q1.usuario = qt1.id limit 20')\n",
        "df_fecha1.show()\n",
        "\n",
        "\n",
        "from pyspark.sql.functions import col, unix_timestamp, to_date\n",
        "df_q=df_q.withColumn('fecha_a', to_date(unix_timestamp(col('creationdate'), 'yyyy-MM-dd').cast('timestamp')))\n",
        "0df_q.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdqizSSQsHtR"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOn-aTiuxA43"
      },
      "source": [
        "**2.2 Origen TWITTER** \n",
        "\n",
        "Mediante flume hacemos una ingesta en tiempo real de los datos de twitter de una lista de lenguajes/tecnologias procedentes de StackOverflow.\n",
        "Para manejar una cantidad representativa de datos hacemos una extración de 22.000 tweets.\n",
        "Nos encontramos con la circunstancia de que los usuarios en Twitter, al menos gran parte de ellos, no indican su país de residencia, lo que impide que el muestreo sea todo lo preciso que deberia ser.\n",
        "\n",
        "Necesitamos saber el Lenguaje y la procedencia de los Tweets extraidos.\n",
        "\n",
        "Usamos la herramienta Apache Flume, para ello hemos creado un fichero de configuración \"twitter.conf\" con los parametros necesarios para realizar la ingesta, entre esos datos indicamos como keywords los lenguajes/tecnologias, claves de acceso a Twitter, configuracion del sumidero, etc...\n",
        "\n",
        "Lanzamos el comando:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3yhHhJd1AXZ"
      },
      "source": [
        "flume-ng agent --conf /etc/flume-ng/conf --conf-file /home/jlgarcia/flume/tweets/twitter.conf --name TwitterAgent --plugins-path /usr/lib/flume-ng/plugins.d:/var/lib/flume-ng/plugins.d -Dflume.root.logger=INFO,console"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tf5nXABR2_Jr"
      },
      "source": [
        "Despues de 15 minutos de proceso, cancelamos para tener una cantidad representativa de Tweets, como no hay forma de saber exactamente cuantos registros lleva extraidos se hace a ojo.\n",
        "\n",
        "Una vez que tenemos los ficheros flume en Hdfs procedemos a crear una sesión es PySpark para hacer el tratamiento de los datos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rq2of2yy3CDx"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = (SparkSession\n",
        "         .builder\n",
        "         .appName(\"Python Spark SQL basic example\")\n",
        "         .config(\"spark.some.config.option\", \"some-value\")\n",
        "         .getOrCreate()\n",
        "         )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yU5d5B3S3My2"
      },
      "source": [
        "Creamos un DataFrame en formato Json con los datos de los ficheros flume\n",
        "\n",
        "*   Elemento de lista\n",
        "*   Elemento de lista\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPHsN1Fr3Xwt"
      },
      "source": [
        "filename = '*'\n",
        "df = spark.read.format('json').option('head',True).load('hdfs:///user/jlgarcia/flume/tweets/' + filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziGK9M4Bsi0A"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fR5HqY9K3agk"
      },
      "source": [
        "Creamos una tabla temporal para poder acceder a los datos json convertidos mediante sintaxis SQL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzdOeC213qjZ"
      },
      "source": [
        "df.registerTempTable('tweets')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IahHqOA33tak"
      },
      "source": [
        "Analizamos la estructura del archivo para localizar los campos que debemos usar para hacer las consultas\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-jFDmeG4RtU"
      },
      "source": [
        "dftweetCompleto = spark.sql(\"SELECT * FROM tweets\")\n",
        "dftweetCompleto.printSchema()\n",
        "\n",
        "root\n",
        " |-- contributors: string (nullable = true)\n",
        " |-- coordinates: struct (nullable = true)\n",
        " |    |-- coordinates: array (nullable = true)\n",
        " |    |    |-- element: double (containsNull = true)\n",
        " |    |-- type: string (nullable = true)\n",
        " |-- created_at: string (nullable = true)\n",
        " |-- display_text_range: array (nullable = true)\n",
        " |    |-- element: long (containsNull = true)\n",
        " |-- entities: struct (nullable = true)\n",
        " |    |-- hashtags: array (nullable = true)\n",
        " |    |    |-- element: struct (containsNull = true)\n",
        " |    |    |    |-- indices: array (nullable = true)\n",
        " |    |    |    |    |-- element: long (containsNull = true)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKLsNoHa4guD"
      },
      "source": [
        "Empezamos con Idioma, por lo tanto usamos : lang\n",
        "\n",
        "> Bloque con sangría\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L28AA9GU4Z2Q"
      },
      "source": [
        "Tmp = spark.sql(\"SELECT lang, count(lang) as contador FROM tweets where lang <> 'und' GROUP BY lang ORDER BY contador DESC\")\n",
        "Tmp.show(100)\n",
        "\n",
        "+----+--------+\n",
        "|lang|contador|\n",
        "+----+--------+\n",
        "|  ja|    8253|\n",
        "|  en|    5516|\n",
        "|  es|    2009|\n",
        "|  fr|     691|\n",
        "|  ko|     483|\n",
        "|  ru|     426|\n",
        "|  de|     374|\n",
        "|  ar|     357|\n",
        "|  tr|     318|\n",
        "|  th|     256|\n",
        "|  pt|     250|\n",
        "|  it|     232|"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6okKm8-4r7p"
      },
      "source": [
        "El segundo dato que necesitamos es el pais, por lo tanto usamos place.country y place.country_code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7b6DXq35MUS"
      },
      "source": [
        "Tmp3 = spark.sql(\"SELECT place.country, place.country_code, count(*) as contador FROM tweets group by place.country, place.country_code\")\n",
        "Tmp3.show(10000)\n",
        "\n",
        "+--------------------+------------+--------+\n",
        "|             country|country_code|contador|\n",
        "+--------------------+------------+--------+\n",
        "|           Indonesia|          ID|       3|\n",
        "|Emiratos Arabes     |          AE|       2|\n",
        "|           Australia|          AU|       3|\n",
        "|            Pakistan|          PK|       1|\n",
        "|              Polska|          PL|       1|\n",
        "|            Colombia|          CO|       1|\n",
        "|            Thailand|          TH|       1|\n",
        "|                null|        null|   22170|\n",
        "|            Malaysia|          MY|       1|\n",
        "|         Switzerland|          CH|       1|\n",
        "|              México|          MX|       1|\n",
        "|              España|          ES|      12|"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7m_KNMM5owR"
      },
      "source": [
        "Una vez que tenemos los resultados de las consultas tenemos que llevarlos a csv para poder importarlos desde \"Tableau\" para poder hacer los graficos.\n",
        "Lo hacemos tanto para los idiomas como para los paises"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuNs0ONe59uX"
      },
      "source": [
        "Tmp.write.format(\"csv\").save('hdfs:///user/jlgarcia/flume/tweets/TweetLang.csv')\n",
        "hdfs dfs -cat /user/jlgarcia/flume/tweets/TweetLang.csv/* > resultado_lang.csv\n",
        "Tmp3.write.format(\"csv\").save('hdfs:///user/jlgarcia/flume/tweets/TweetCountry.csv')\n",
        "hdfs dfs -cat /user/jlgarcia/flume/tweets/TweetCountry2.csv/* > resultado_country.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8XXA5hP7B5_"
      },
      "source": [
        "Importamos los ficheros \"resultado_lang.csv\" y \"resultado_country.csv\" en Tableau y hacemos los graficos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MazNl1_itXVv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XACCTg5D4Di"
      },
      "source": [
        "### 3. Resultados\n",
        "\n",
        "•\tLenguajes y tecnologías más populares\n",
        "\n",
        "Como vimos anteriormente, el conteo general de las tecnologías y herramientas más demandadas sería el siguiente:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuYhuvBUsYgL"
      },
      "source": [
        "Y con el criterio de ordenación por fecha:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rf5RJ36D1VM-"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKVzdMgKr9Nx"
      },
      "source": [
        "•\tPopularidad en función del país"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPp6yswLsAuI"
      },
      "source": [
        "•\tIdioma de quien utiliza las herramientas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFhNVPOUEDaj"
      },
      "source": [
        "* Gráfico con 10 tecnologías más populares y su tendencia desde 2008 hasta 2017"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaXI3tAoTSJb"
      },
      "source": [
        "##Ingesta de datos con sqoop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxxZ3yb658uQ"
      },
      "source": [
        "sqoop import-all-tables --connect jdbc:mysql://10.0.4.201:5432/stacklite \\--username training \\--password training \\--target-dir /user/jlgarcia/grupob --verbose\n",
        "\n",
        "sqoop import --connect jdbc:mysql://10.0.4.201:3306/stacklite --username training --password training --table question_tags --delete-target-dir --target-dir \"/user/jlgarcia/grupob\" --verbose"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKo9inhDqZbd"
      },
      "source": [
        "## importar con sqoop a través de Hive\n",
        "\n",
        "sqoop import --connect jdbc:mysql://10.0.4.201:3306/stacklite --username training --password training --table question_tags --hive-import ((((con mi usuario -p))))\n",
        "\n",
        "sqoop import --connect jdbc:mysql://10.0.4.201:3306/stacklite --username training --password training --table questions --hive-import"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97MoMZsT1Jpu"
      },
      "source": [
        "Archivo de configuración del agente flume Twitter\n",
        "\n",
        "TwitterAgent.sources = Twitter\n",
        "TwitterAgent.channels = MemChannel\n",
        "TwitterAgent.sinks = HDFS\n",
        "\n",
        "#TwitterAgent.sources.Twitter.type = com.cloudera.flume.source.TwitterSource\n",
        "TwitterAgent.sources.Twitter.type = org.apache.flume.source.twitter.TwitterSource\n",
        "TwitterAgent.sources.Twitter.consumerKey = wWfHmif7iRq1h4GtzdhOUhHMc\n",
        "TwitterAgent.sources.Twitter.consumerSecret = Vg2bKZkq3uxRqNihlEpZxzixwjvClxDzIshshO6k5K5du0gMvN\n",
        "TwitterAgent.sources.Twitter.accessToken = 289473525-vurwFr068lcdKWQFodMcsqQAzLMCeq0w6ndMcvq6\n",
        "TwitterAgent.sources.Twitter.accessTokenSecret = pHBak5EmBvW6iLPYRiSwKAO0b2ztgojC5ou4QoVvv0Lnt\n",
        "TwitterAgent.sources.Twitter.keywords = python\n",
        "#c#, c++, c, java, javascript, perl, ada, lisp, vb.net, pascal, cobol, php, pl/sql, ruby, fortran, algol, erlang\n",
        "\n",
        "TwitterAgent.sinks.HDFS.channel = MemChannel\n",
        "TwitterAgent.sinks.HDFS.type = hdfs\n",
        "TwitterAgent.sinks.HDFS.hdfs.path = /user/jlgarcia/flume/tweets\n",
        "TwitterAgent.sinks.HDFS.hdfs.fileType = DataStream\n",
        "TwitterAgent.sinks.HDFS.hdfs.writeFormat = Text\n",
        "TwitterAgent.sinks.HDFS.hdfs.batchSize = 1000\n",
        "TwitterAgent.sinks.HDFS.hdfs.rollSize = 0\n",
        "TwitterAgent.sinks.HDFS.hdfs.rollCount = 1000\n",
        "TwitterAgent.sinks.HDFS.hdfs.rollInterval = 600\n",
        "\n",
        "TwitterAgent.channels.MemChannel.type = memory\n",
        "TwitterAgent.channels.MemChannel.capacity = 10000\n",
        "TwitterAgent.channels.MemChannel.transactionCapacity = 10000\n",
        "\n",
        "TwitterAgent.sources.Twitter.channels = MemChannel\n",
        "TwitterAgent.sinks.HDFS.channel = MemChannel\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uq4zXH_xSBlv"
      },
      "source": [
        "Archivo de configuración del agente flume Twitter\n",
        "\n",
        "TwitterAgent.sources = Twitter\n",
        "TwitterAgent.channels = MemChannel\n",
        "TwitterAgent.sinks = HDFS\n",
        "\n",
        "#TwitterAgent.sources.Twitter.type = com.cloudera.flume.source.TwitterSource\n",
        "TwitterAgent.sources.Twitter.type = org.apache.flume.source.twitter.TwitterSource\n",
        "TwitterAgent.sources.Twitter.consumerKey = wWfHmif7iRq1h4GtzdhOUhHMc\n",
        "TwitterAgent.sources.Twitter.consumerSecret = Vg2bKZkq3uxRqNihlEpZxzixwjvClxDzIshshO6k5K5du0gMvN\n",
        "TwitterAgent.sources.Twitter.accessToken = 289473525-vurwFr068lcdKWQFodMcsqQAzLMCeq0w6ndMcvq6\n",
        "TwitterAgent.sources.Twitter.accessTokenSecret = pHBak5EmBvW6iLPYRiSwKAO0b2ztgojC5ou4QoVvv0Lnt\n",
        "TwitterAgent.sources.Twitter.keywords = python\n",
        "#c#, c++, c, java, javascript, perl, ada, lisp, vb.net, pascal, cobol, php, pl/sql, ruby, fortran, algol, erlang\n",
        "\n",
        "TwitterAgent.sinks.HDFS.channel = MemChannel\n",
        "TwitterAgent.sinks.HDFS.type = hdfs\n",
        "TwitterAgent.sinks.HDFS.hdfs.path = /user/jlgarcia/flume/tweets\n",
        "TwitterAgent.sinks.HDFS.hdfs.fileType = DataStream\n",
        "TwitterAgent.sinks.HDFS.hdfs.writeFormat = Text\n",
        "TwitterAgent.sinks.HDFS.hdfs.batchSize = 1000\n",
        "TwitterAgent.sinks.HDFS.hdfs.rollSize = 0\n",
        "TwitterAgent.sinks.HDFS.hdfs.rollCount = 1000\n",
        "TwitterAgent.sinks.HDFS.hdfs.rollInterval = 600\n",
        "\n",
        "TwitterAgent.channels.MemChannel.type = memory\n",
        "TwitterAgent.channels.MemChannel.capacity = 10000\n",
        "TwitterAgent.channels.MemChannel.transactionCapacity = 10000\n",
        "\n",
        "TwitterAgent.sources.Twitter.channels = MemChannel\n",
        "TwitterAgent.sinks.HDFS.channel = MemChannel\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPRbBeIQSfNz"
      },
      "source": [
        "Comando para lanzar agente flume que carga Twitter\n",
        "\n",
        "flume-ng agent -n TwitterAgent -c conf -f /home/jlgarcia/flume/tweets/twitter.conf -Dflume.root.logger=INFO, console"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEtPFYMdQ2ue"
      },
      "source": [
        "https://www.toptal.com/apache/tutorial-apache-spark-streaming-identificando-los-hashtags-de-tendencia-de-twitter/es"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vCp8Qlw3Ezz"
      },
      "source": [
        "## **mysql esquema de acciones**\n",
        "(https://drive.google.com/open?id=0B0p-gxIq9qEuQUFMSjY3Q3NVcElLLTI2VW5ERzFUck5RUnlF)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EP0ZmmABNOvQ"
      },
      "source": [
        "https://archive.org/details/stackexchange"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnquXj_Whddc"
      },
      "source": [
        "SELECT * FROM project_questions, project_question_tags\n",
        "WHERE project_questions.usuario = project_question_tags.id;"
      ]
    }
  ]
}